# -*- coding: utf-8 -*-
"""toy_kd.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qs0RyfhP1o3J-_sGH92QvdcHln084Ow6
"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import matplotlib
import math
import numpy as np
import matplotlib.pyplot as plt
import sklearn.datasets
from tqdm import tqdm
import pandas as pd
import os

import torch
from torch.nn.parameter import Parameter
import torch.nn as nn
import torch.nn.functional as F
from torch.utils import data


def plot_half_moon(X, y):
    #from https://github.com/dennybritz/nn-from-scratch/blob/master/nn-from-scratch.ipynb
    # Set min and max values and give it some padding
    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5
    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5
    
    h = 0.01
    # Generate a grid of points with distance h between them
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
    
    yy = yy.astype('float32')
    xx = xx.astype('float32')
    # Predict the function value for the whole gid
    # y = y.reshape(xx.shape)
    # Plot the contour and training examples
    plt.figure()
    # plt.contourf(xx, yy, Z, cmap=plt.cm.RdBu)
    plt.scatter(X[:, 0], X[:, 1], c=-y, cmap=plt.cm.Spectral)


@torch.no_grad()
def plot_decision_boundary(model, X, y):
    #from https://github.com/dennybritz/nn-from-scratch/blob/master/nn-from-scratch.ipynb
    # Set min and max values and give it some padding
    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5
    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5
    
    h = 0.01
    # Generate a grid of points with distance h between them
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
    
    yy = yy.astype('float32')
    xx = xx.astype('float32')
    # Predict the function value for the whole gid
    xs = torch.tensor(np.c_[xx.ravel(), yy.ravel()]).cuda()
    Z = model(xs)[:,0].cpu().numpy()
    Z = Z.reshape(xx.shape)
    # Plot the contour and training examples
    plt.figure()
    plt.contourf(xx, yy, Z, cmap=plt.cm.RdBu)
    plt.scatter(X[:, 0], X[:, 1], c=-y, cmap=plt.cm.Spectral)

class Halfmoon(data.Dataset):
    def __init__(self, X, y):
        self.X = X.astype("float32")
        self.y = y.astype("float32")
        
    def __getitem__(self, index):
        return self.X[index], int(self.y[index])
    
    def __len__(self):
        return len(self.X)

class AverageMeter(object):
    """Computes and stores the average and current value
       Imported from https://github.com/pytorch/examples/blob/master/imagenet/main.py#L247-L262
    """
    def __init__(self):
        self.reset()

    def reset(self):
        self.val = 0
        self.avg = 0
        self.sum = 0
        self.count = 0

    def update(self, val, n=1):
        self.val = val
        self.sum += val * n
        self.count += n
        self.avg = self.sum / self.count

def accuracy(output, target, topk=(1,)):
    """Computes the precision@k for the specified values of k"""
    maxk = max(topk)
    maxk = min(output.shape[-1], maxk)

    batch_size = target.size(0)

    _, pred = output.topk(maxk, 1, True, True)
    pred = pred.t()
    correct = pred.eq(target.view(1, -1).expand_as(pred))

    res = []
    for k in topk:
        correct_k = correct[:k].reshape(-1).float().sum(0)
        res.append(correct_k.mul_(100.0 / batch_size))
    return res

class Network(nn.Module):

    def __init__(self, layer_size=10):
        super(Network, self).__init__()
        self.layer = nn.Sequential(
            nn.Linear(2, layer_size),
            nn.ReLU(),
            nn.Linear(layer_size, 3)
        )
        
    def forward(self, x):
        return self.layer(x)

def train(trainloader, model, optimizer, criterion):
    # switch to train mode
    model.train()

    losses = AverageMeter()
    top1 = AverageMeter()

    # bar = tqdm(enumerate(trainloader), total=len(trainloader))
    for batch_idx, (inputs, targets) in enumerate(trainloader):
        
        inputs, targets = inputs.cuda(), targets.cuda()
        inputs = inputs.float()

        # compute output
        outputs = model(inputs)
        loss = criterion(outputs, targets)

        # measure accuracy and record loss
        prec1, = accuracy(outputs.data, targets.data, topk=(1, ))
        losses.update(loss.item(), inputs.size(0))

        top1.update(prec1.item(), inputs.size(0))

        # compute gradient and do SGD step
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        # plot progress
        # bar.set_postfix_str('({batch}/{size}) Loss: {loss:.8f} | top1: {top1: .4f}'.format(
        #             batch=batch_idx + 1,
        #             size=len(trainloader),
        #             loss=losses.avg,
        #             top1=top1.avg
        #             ))

    return (losses.avg, top1.avg)

@torch.no_grad()
def test(testloader, model, criterion):

    losses = AverageMeter()
    top1 = AverageMeter()

    # switch to evaluate mode
    model.eval()

    # bar = tqdm(enumerate(testloader), total=len(testloader))
    for batch_idx, (inputs, targets) in enumerate(testloader):

        inputs, targets = inputs.cuda(), targets.cuda()
        inputs = inputs.float()

        # compute output
        outputs = model(inputs)
        loss = criterion(outputs, targets)
        
        prec1, prec3, prec5  = accuracy(outputs.data, targets.data, topk=(1, 3, 5))
        losses.update(loss.item(), inputs.size(0))
        top1.update(prec1.item(), inputs.size(0))

        # plot progress
        # bar.set_postfix_str('({batch}/{size}) Loss: {loss:.8f} | top1: {top1: .4f}'.format(
        #             batch=batch_idx + 1,
        #             size=len(testloader),
        #             loss=losses.avg,
        #             top1=top1.avg,
        #             ))

    return (losses.avg, top1.avg)

def train_teacher(X_train, y_train, X_test, y_test, hidden_size):
    train_dataset = Halfmoon(X_train, y_train)
    train_loader = data.DataLoader(train_dataset, batch_size=32, num_workers=4)

    test_dataset = Halfmoon(X_test, y_test)
    test_loader = data.DataLoader(test_dataset, batch_size=32, num_workers=4)

    criterion = torch.nn.CrossEntropyLoss()
    test_criterion = torch.nn.CrossEntropyLoss()

    lr = 0.01
    momentum = 0.9
    weight_decay = 0.0001
    epochs = 100

    teacher = Network(layer_size=hidden_size)
    teacher.cuda()
    # print(teacher)

    optimizer = torch.optim.SGD(teacher.parameters(), 
                                lr=lr, 
                                momentum=momentum, 
                                weight_decay=weight_decay)

    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[int(0.5 * epochs), int(0.75 * epochs)])

    bar = tqdm(range(epochs))
    for epoch in bar:
        train_loss, top1_train = train(train_loader, teacher, optimizer, criterion)
        test_loss, top1 = test(test_loader, teacher, test_criterion)

        bar.set_postfix_str('lr : {lr:.5f} | train_loss: {train_loss:.5f} | test_loss: {test_loss:.5f} | top1: {top1: .4f}'.format(
                        train_loss=train_loss,
                        test_loss=test_loss,
                        top1=top1,
                        lr=scheduler.get_last_lr()[0]
                        ))

        scheduler.step()
    return teacher

class VanillaKD(nn.Module):
    def __init__(self, temp=5.0, distil_weight=0.99) -> None:
        super().__init__()
        self.temp = temp
        self.distil_weight = distil_weight
        self.cross_entropy = nn.CrossEntropyLoss()
        self.kl_loss = nn.KLDivLoss(reduction="batchmean")

    def forward(self, student_output, teacher_output, labels):
        alpha = self.distil_weight
        T = self.temp
        KD_loss = self.kl_loss(F.log_softmax(student_output/T, dim=1),
                                F.softmax(teacher_output/T, dim=1)) * (alpha * T * T) + \
                F.cross_entropy(student_output, labels) * (1. - alpha)

        return KD_loss

def train_student_(trainloader, student, teacher, optimizer, criterion):
    # switch to train mode
    student.train()
    teacher.eval()

    losses = AverageMeter()
    top1 = AverageMeter()

    for batch_idx, (inputs, targets) in enumerate(trainloader):
        
        inputs, targets = inputs.cuda(), targets.cuda()

        # compute output
        outputs = student(inputs)
        with torch.no_grad():
            outputs_teacher = teacher(inputs)

        loss = criterion(outputs, outputs_teacher, targets)

        # measure accuracy and record loss
        prec1, = accuracy(outputs.data, targets.data, topk=(1, ))
        losses.update(loss.item(), inputs.size(0))

        top1.update(prec1.item(), inputs.size(0))

        # compute gradient and do SGD step
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        # plot progress

    return (losses.avg, top1.avg)

def train_student(teacher, X_train, y_train, X_test, y_test, hidden_size):
    train_dataset = Halfmoon(X_train, y_train)
    train_loader = data.DataLoader(train_dataset, batch_size=32, num_workers=4)

    test_dataset = Halfmoon(X_test, y_test)
    test_loader = data.DataLoader(test_dataset, batch_size=32, num_workers=4)

    temp = 5.0
    dw = 0.99

    criterion = VanillaKD(temp=temp, distil_weight=dw)
    test_criterion = torch.nn.CrossEntropyLoss()

    lr = 0.01
    momentum = 0.9
    weight_decay = 0.0001
    epochs = 100

    student = Network(layer_size=hidden_size)
    student.cuda()

    optimizer = torch.optim.SGD(student.parameters(), 
                                lr=lr, 
                                momentum=momentum, 
                                weight_decay=weight_decay)

    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[int(0.5 * epochs), int(0.75 * epochs)])

    bar = tqdm(range(epochs))
    for epoch in bar:
        train_loss, top1_train = train_student_(train_loader, student, teacher, optimizer, criterion)
        test_loss, top1 = test(test_loader, student, test_criterion)

        bar.set_postfix_str('lr : {lr:.5f} | train_loss: {train_loss:.5f} | test_loss: {test_loss:.5f} | top1: {top1: .4f}'.format(
                        train_loss=train_loss,
                        test_loss=test_loss,
                        top1=top1,
                        lr=scheduler.get_last_lr()[0]
                        ))
        
        scheduler.step()
    
    return student

def diff_l2(teacher, student):
    total = 0.
    for p1, p2 in zip(teacher.parameters(), student.parameters()):
        teacher_tensor = p1.data
        student_tensor = p2.data
        diff = torch.sum((teacher_tensor - student_tensor)**2)
        total += diff.item()
    return math.sqrt(total)

# input size does not change
# torch.backends.cudnn.benchmark = True
N = 1200
noise = 0.3
hidden_sizes = [16, 32, 64]

for hidden_size in hidden_sizes:
    for i in range(10):
        print("generating dataset...")
        # X, y = sklearn.datasets.make_moons(n_samples=N, noise=noise, random_state=i, shuffle=True)
        X, y = sklearn.datasets.make_blobs(n_samples=N, n_features=2, centers=3, shuffle=True, random_state=i)

        split_no = int(0.8 * N)
        X_train, y_train = X[:split_no], y[:split_no]
        X_test, y_test = X[split_no:], y[split_no:]

        # train 3 teachers, then train 3 students
        print("training teacher-student pair...")
        torch.manual_seed(i*5)
        teacher1 = train_teacher(X_train, y_train, X_test, y_test, hidden_size)
        student1 = train_student(teacher1, X_train, y_train, X_test, y_test, hidden_size)

        print("training teacher-student pair...")
        torch.manual_seed(i*5+1)
        teacher2 = train_teacher(X_train, y_train, X_test, y_test, hidden_size)
        student2 = train_student(teacher2, X_train, y_train, X_test, y_test, hidden_size)

        print("training teacher-student pair...")
        torch.manual_seed(i*5+2)
        teacher3 = train_teacher(X_train, y_train, X_test, y_test, hidden_size)
        student3 = train_student(teacher3, X_train, y_train, X_test, y_test, hidden_size)

        teachers = [teacher1, teacher2, teacher3]
        students = [student1, student2, student3]

        df = { "hidden_size" : [hidden_size], "N" : [N], "noise" : [noise] }

        for i, x in enumerate(teachers):
            for j, y in enumerate(students):
                # append to the csv file
                df[str(i) + str(j)] = [diff_l2(x, y)]
        
        df =  pd.DataFrame(df)
        save_path = os.path.join("results", "l2_norms.csv")
        df.to_csv(save_path, mode='a', index=False, header=(not os.path.exists(save_path)))
