INFO:  Setting up logging folder : trained_model_library/cifar10/convnet10/2022-12-30-22:41:31.127911_NLL+MDCA_beta=1.0
INFO:  Namespace(Lambda=0.9, T=20, alpha=5.0, beta=1.0, chain_length=None, checkpoint='trained_model_library', current_time='', dataset='cifar10', epochs=1, exp_name='', gamma=1, loss='NLL+MDCA', lr=0.1, lr_decay_factor=0.1, model='convnet10', momentum=0.9, optimizer='sgd', patience=10, prefix='', regularizer='l2', schedule_steps=[80, 120], scheduler='multistep', seed=42, start_epoch=0, teacher='', teacher_loss='cross_entropy', teacher_path='', test_batch_size=100, train_batch_size=128, warmup=0, weight_decay=0.0001, workers=4)
INFO:  ['train_scratch.py', '--dataset', 'cifar10', '--model', 'convnet10', '--lr', '0.1', '--epochs', '1', '--scheduler', 'multistep', '--schedule-steps', '80', '120', '--lr-decay-factor', '0.1', '--wd', '1e-4', '--train-batch-size', '128', '--loss', 'NLL+MDCA', '--beta', '1.0', '--checkpoint', 'trained_model_library']
INFO:  GPUs used: 1
INFO:  Using model : convnet10
INFO:  Using dataset : cifar10
INFO:  Setting up optimizer : sgd
INFO:  using loss = NLL+MDCA
INFO:  using NLL + (beta=1.0) mdca
INFO:  Step sizes : [80, 120] | lr-decay-factor : 0.1
