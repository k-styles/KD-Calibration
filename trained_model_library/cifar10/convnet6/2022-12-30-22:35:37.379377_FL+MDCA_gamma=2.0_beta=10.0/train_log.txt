INFO:  Setting up logging folder : trained_model_library/cifar10/convnet6/2022-12-30-22:35:37.379377_FL+MDCA_gamma=2.0_beta=10.0
INFO:  Namespace(Lambda=0.9, T=20, alpha=5.0, beta=10.0, chain_length=None, checkpoint='trained_model_library', current_time='', dataset='cifar10', epochs=1, exp_name='', gamma=2.0, loss='FL+MDCA', lr=0.1, lr_decay_factor=0.1, model='convnet6', momentum=0.9, optimizer='sgd', patience=10, prefix='', regularizer='l2', schedule_steps=[80, 120], scheduler='multistep', seed=42, start_epoch=0, teacher='', teacher_loss='cross_entropy', teacher_path='', test_batch_size=100, train_batch_size=128, warmup=0, weight_decay=0.0001, workers=4)
INFO:  ['train_scratch.py', '--dataset', 'cifar10', '--model', 'convnet6', '--lr', '0.1', '--epochs', '1', '--scheduler', 'multistep', '--schedule-steps', '80', '120', '--lr-decay-factor', '0.1', '--wd', '1e-4', '--train-batch-size', '128', '--loss', 'FL+MDCA', '--gamma', '2.0', '--beta', '10.0', '--checkpoint', 'trained_model_library']
INFO:  GPUs used: 1
INFO:  Using model : convnet6
INFO:  Using dataset : cifar10
INFO:  Setting up optimizer : sgd
INFO:  using loss = FL+MDCA
INFO:  using gamma=2.0
INFO:  using FL (gamma=2.0) + (beta=10.0) mdca
INFO:  Step sizes : [80, 120] | lr-decay-factor : 0.1
